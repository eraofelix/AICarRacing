import random
import time
from collections import defaultdict, deque
from typing import Tuple, List, Dict
import pdb

def draw_card(rng: random.Random) -> int:
    card = rng.randint(1, 13)
    return min(card, 10)

def draw_hand(rng: random.Random) -> List[int]:
    return [draw_card(rng), draw_card(rng)]

def usable_ace(hand: List[int]) -> bool:
    return 1 in hand and sum(hand) + 10 <= 21

def hand_value(hand: List[int]) -> int:
    total = sum(hand)
    if usable_ace(hand):
        return total + 10
    return total

def is_bust(hand: List[int]) -> bool:
    return hand_value(hand) > 21

def dealer_policy(dealer: List[int], rng: random.Random) -> List[int]:
    while hand_value(dealer) < 17:
        dealer.append(draw_card(rng))
    return dealer

State = Tuple[int, int, bool]
Action = int  # 0=stick, 1=hit

class MCAgentES:
    def __init__(self, seed: int = 0, epsilon: float = 0.1):
        self.rng = random.Random(seed)
        self.Q: Dict[State, Dict[Action, float]] = defaultdict(lambda: {0: 0.0, 1: 0.0})
        self.returns_count: Dict[Tuple[State, Action], int] = defaultdict(int)
        self.epsilon = epsilon

    def policy(self, s: State) -> Action:
        if self.rng.random() < self.epsilon:
            return self.rng.choice([0, 1])
        q0, q1 = self.Q[s][0], self.Q[s][1]
        return 0 if q0 >= q1 else 1

def play_one_episode_with_es(agent: MCAgentES, rng: random.Random) -> Tuple[List[Tuple[State, Action]], float]:
    # æ¢ç´¢å¼€å§‹ï¼šéšæœºé€‰æ‹©åˆå§‹çŠ¶æ€-åŠ¨ä½œå¯¹
    print("ğŸ² å¼€å§‹æ–°çš„episode - æ¢ç´¢å¼€å§‹æ¨¡å¼")
    while True:
        target_sum = rng.randint(12, 21)
        dealer_show = rng.randint(1, 10)
        ua = bool(rng.randint(0, 1))
        print(f"ğŸ¯ ç›®æ ‡çŠ¶æ€: ç©å®¶ç‚¹æ•°={target_sum}, åº„å®¶æ˜ç‰Œ={dealer_show}, è½¯A={ua}")
        
        # ç”Ÿæˆå¯è¡Œçš„åˆå§‹çŠ¶æ€
        success = False
        for _ in range(5000):
            player = draw_hand(rng)
            for _ in range(rng.randint(0, 3)):
                if hand_value(player) < 21:
                    player.append(draw_card(rng))
            if 12 <= hand_value(player) <= 21 and usable_ace(player) == ua and hand_value(player) == target_sum:
                success = True
                break
        if not success:
            print("âŒ æ— æ³•ç”Ÿæˆç›®æ ‡ç©å®¶æ‰‹ç‰Œï¼Œé‡æ–°å°è¯•...")
            continue
            
        # ç”Ÿæˆå¯è¡Œçš„åº„å®¶ç‰Œ
        for _ in range(5000):
            dealer = draw_hand(rng)
            if dealer[0] == dealer_show or dealer[1] == dealer_show:
                if dealer[0] != dealer_show:
                    dealer = [dealer[1], dealer[0]]
                break
            else:
                continue
        break
    
    print(f"âœ… æˆåŠŸç”Ÿæˆåˆå§‹çŠ¶æ€: ç©å®¶={player}({hand_value(player)}), åº„å®¶={dealer}({dealer[0]}æ˜ç‰Œ)")

    s = (hand_value(player), dealer[0], usable_ace(player))
    a = rng.choice([0, 1])
    action_name = "åœç‰Œ" if a == 0 else "è¦ç‰Œ"
    print(f"ğŸ¯ æ¢ç´¢å¼€å§‹åŠ¨ä½œ: {action_name} (éšæœºé€‰æ‹©)")

    episode_sa: List[Tuple[State, Action]] = []
    episode_sa.append((s, a))

    # æ‰§è¡Œç¬¬ä¸€ä¸ªåŠ¨ä½œ
    print(f"ğŸƒ æ‰§è¡ŒåŠ¨ä½œ: {action_name}")
    if a == 1:
        player.append(draw_card(rng))
        print(f"ğŸ“ˆ è¦ç‰Œå: ç©å®¶={player}({hand_value(player)})")
        if is_bust(player):
            print("ğŸ’¥ ç©å®¶çˆ†ç‰Œï¼æ¸¸æˆç»“æŸï¼Œå¥–åŠ±=-1")
            return episode_sa, -1.0
    else:
        print("ğŸ›‘ ç©å®¶åœç‰Œï¼Œåº„å®¶å¼€å§‹è¡ŒåŠ¨...")
        dealer_policy(dealer, rng)
        print(f"ğŸ¦ åº„å®¶æœ€ç»ˆæ‰‹ç‰Œ: {dealer}({hand_value(dealer)})")
        if is_bust(dealer):
            print("ğŸ‰ åº„å®¶çˆ†ç‰Œï¼ç©å®¶è·èƒœï¼Œå¥–åŠ±=+1")
            return episode_sa, +1.0
        pv, dv = hand_value(player), hand_value(dealer)
        result = 1 if pv > dv else -1 if pv < dv else 0
        result_text = "ç©å®¶è·èƒœ" if result > 0 else "åº„å®¶è·èƒœ" if result < 0 else "å¹³å±€"
        print(f"ğŸ† æœ€ç»ˆç»“æœ: ç©å®¶={pv}, åº„å®¶={dv}, {result_text}, å¥–åŠ±={result}")
        return episode_sa, float(result)

    # ç»§ç»­ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥
    print("ğŸ”„ ç»§ç»­æ¸¸æˆï¼Œä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥...")
    while True:
        s = (hand_value(player), dealer[0], usable_ace(player))
        a = agent.policy(s)
        action_name = "åœç‰Œ" if a == 0 else "è¦ç‰Œ"
        print(f"ğŸ¤– æ™ºèƒ½ä½“å†³ç­–: çŠ¶æ€={s}, åŠ¨ä½œ={action_name}")
        episode_sa.append((s, a))
        if a == 1:
            player.append(draw_card(rng))
            print(f"ğŸ“ˆ è¦ç‰Œå: ç©å®¶={player}({hand_value(player)})")
            if is_bust(player):
                print("ğŸ’¥ ç©å®¶çˆ†ç‰Œï¼æ¸¸æˆç»“æŸï¼Œå¥–åŠ±=-1")
                return episode_sa, -1.0
        else:
            print("ğŸ›‘ ç©å®¶åœç‰Œï¼Œåº„å®¶å¼€å§‹è¡ŒåŠ¨...")
            dealer_policy(dealer, rng)
            print(f"ğŸ¦ åº„å®¶æœ€ç»ˆæ‰‹ç‰Œ: {dealer}({hand_value(dealer)})")
            if is_bust(dealer):
                print("ğŸ‰ åº„å®¶çˆ†ç‰Œï¼ç©å®¶è·èƒœï¼Œå¥–åŠ±=+1")
                return episode_sa, +1.0
            pv, dv = hand_value(player), hand_value(dealer)
            result = 1 if pv > dv else -1 if pv < dv else 0
            result_text = "ç©å®¶è·èƒœ" if result > 0 else "åº„å®¶è·èƒœ" if result < 0 else "å¹³å±€"
            print(f"ğŸ† æœ€ç»ˆç»“æœ: ç©å®¶={pv}, åº„å®¶={dv}, {result_text}, å¥–åŠ±={result}")
            return episode_sa, float(result)

def derive_greedy_policy(Q: Dict[State, Dict[Action, float]]) -> Dict[State, Action]:
    policy = {}
    for s, q in Q.items():
        policy[s] = 0 if q[0] >= q[1] else 1
    return policy

def policy_diff_rate(pi_a: Dict[State, Action], pi_b: Dict[State, Action]) -> float:
    if not pi_a and not pi_b:
        return 0.0
    keys = set(pi_a.keys()) | set(pi_b.keys())
    if not keys:
        return 0.0
    diff = sum(1 for k in keys if pi_a.get(k) != pi_b.get(k))
    return diff / len(keys)

# -------------------------
# MC Control with ES + progress
# -------------------------

def mc_control_exploring_starts(
    num_episodes: int = 500_000,
    epsilon: float = 0.1,
    seed: int = 123,
    log_every: int = 10_000,
    recent_window: int = 50_000,
    verbose: bool = True
) -> Tuple[Dict[State, Dict[Action, float]], Dict[State, int]]:
    rng = random.Random(seed)
    agent = MCAgentES(seed=seed, epsilon=epsilon)

    # Rolling performance estimates
    rewards_window = deque(maxlen=recent_window)
    start_time = time.time()

    last_policy_snapshot: Dict[State, Action] = {}
    last_log_time = start_time

    print(f"ğŸš€ å¼€å§‹è’™ç‰¹å¡æ´›æ¢ç´¢å¼€å§‹è®­ç»ƒï¼Œæ€»episodes: {num_episodes:,}")
    print("=" * 60)
    
    for ep in range(1, num_episodes + 1):
        print(f"\n --------------------------ğŸ“Š Episode {ep}/{num_episodes}--------------------------")
        episode_sa, G = play_one_episode_with_es(agent, rng)
        
        print(f"ğŸ“ˆ Episode {ep} ç»“æœ: å¥–åŠ±={G}, çŠ¶æ€-åŠ¨ä½œåºåˆ—é•¿åº¦={len(episode_sa)}")
        
        rewards_window.append(G)
        if ep % 100000 == 0:
            avg_visits = sum(agent.returns_count.values()) / max(1, len(agent.returns_count))
            print(f"ğŸ“Š Episode {ep}/{num_episodes}, ä¸åŒçŠ¶æ€-åŠ¨ä½œå¯¹: {len(agent.returns_count)}, å¹³å‡è®¿é—®æ¬¡æ•°: {avg_visits:.2f}")
        
        # æ›´æ–°Qå€¼è¡¨
        visited = set()
        for (s, a) in episode_sa:
            if (s, a) in visited:
                continue
            visited.add((s, a))
            agent.returns_count[(s, a)] += 1
            n = agent.returns_count[(s, a)]
            q_old = agent.Q[s][a]
            agent.Q[s][a] = q_old + (G - q_old) / n  # å¢é‡å¹³å‡ï¼Œæ— éœ€å­˜å‚¨å†å²æ ·æœ¬çœå†…å­˜
            
            print(f"ğŸ”„ æ›´æ–°Qå€¼: çŠ¶æ€={s}, åŠ¨ä½œ={a}, æ—§å€¼={q_old:.3f}, æ–°å€¼={agent.Q[s][a]:.3f}, è®¿é—®æ¬¡æ•°={n}")

        if verbose and (ep % log_every == 0 or ep == 1):
            now = time.time()
            elapsed = now - start_time
            since_last = now - last_log_time
            last_log_time = now

            # Stats
            distinct_sa = len(agent.returns_count)
            avg_visits = sum(agent.returns_count.values()) / max(1, distinct_sa)
            mean_recent = sum(rewards_window) / max(1, len(rewards_window))
            win_rate = (sum(1 for r in rewards_window if r > 0) / max(1, len(rewards_window)))
            lose_rate = (sum(1 for r in rewards_window if r < 0) / max(1, len(rewards_window)))
            draw_rate = 1.0 - win_rate - lose_rate

            # Policy stability
            current_policy = derive_greedy_policy(agent.Q)
            diff_rate = policy_diff_rate(current_policy, last_policy_snapshot)
            last_policy_snapshot = current_policy

            # Throughput
            eps_per_sec = log_every / max(1e-9, since_last) if ep > 1 else ep / max(1e-9, elapsed)
            eta = (num_episodes - ep) / max(1e-9, eps_per_sec)

            msg = (
                f"[{ep:,}/{num_episodes:,}] "
                f"elapsed={elapsed:,.1f}s, eta={eta:,.1f}s, "
                f"eps/s={eps_per_sec:,.0f}, "
                f"distinct(s,a)={distinct_sa:,}, avg_visits={avg_visits:.2f}, "
                f"recent_win={win_rate:.3f}, lose={lose_rate:.3f}, draw={draw_rate:.3f}, "
                f"policy_change={diff_rate:.3f}"
            )
            print(msg, flush=True)

    # Aggregate state visits
    state_visits: Dict[State, int] = defaultdict(int)
    for (s, a), c in agent.returns_count.items():
        state_visits[s] += c

    return agent.Q, state_visits

def print_policy(policy: Dict[State, Action]):
    print("\nğŸ¯ å­¦ä¹ åˆ°çš„æœ€ä¼˜ç­–ç•¥:")
    print("=" * 50)
    print("è¯´æ˜: S=åœç‰Œ(Stick), H=è¦ç‰Œ(Hit)")
    print("PS=ç©å®¶ç‚¹æ•°, DS=åº„å®¶æ˜ç‰Œ")
    print()
    
    def render(ua: bool):
        ace_type = "æœ‰è½¯A" if ua else "æ— è½¯A"
        print(f"ğŸ“‹ ç­–ç•¥è¡¨ (usable_ace={ua}) - {ace_type}")
        header = "PS\\DS | " + " ".join(f"{d:2d}" for d in range(1, 11))
        print(header)
        print("-" * len(header))
        for ps in range(21, 11, -1):
            row = [f"{ps:2d}  |"]
            for ds in range(1, 11):
                a = policy.get((ps, ds, ua), 0)
                row.append(" S" if a == 0 else " H")
            print(" ".join(row))
        print()

    render(False)
    render(True)

if __name__ == "__main__":
    print("ğŸ® 21ç‚¹æ¸¸æˆè’™ç‰¹å¡æ´›æ¢ç´¢å¼€å§‹ç®—æ³•")
    print("=" * 60)
    print("ç®—æ³•è¯´æ˜:")
    print("1. ğŸ¯ æ¢ç´¢å¼€å§‹: æ¯ä¸ªepisodeä»éšæœºçŠ¶æ€-åŠ¨ä½œå¯¹å¼€å§‹")
    print("2. ğŸ§  Îµ-è´ªå©ªç­–ç•¥: å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨")
    print("3. ğŸ“Š è’™ç‰¹å¡æ´›æ–¹æ³•: é€šè¿‡å¤§é‡é‡‡æ ·ä¼°è®¡ä»·å€¼å‡½æ•°")
    print("4. ğŸ† ç›®æ ‡: å­¦ä¹ æœ€ä¼˜çš„21ç‚¹æ¸¸æˆç­–ç•¥")
    print("=" * 60)
    
    NUM_EPISODES = 300000
    EPSILON = 0.1
    SEED = 2025
    LOG_EVERY = 100  # å‡å°‘æ—¥å¿—é—´éš”ç”¨äºæ¼”ç¤º
    RECENT_WINDOW = 50_000

    print(f"ğŸ”§ è®­ç»ƒå‚æ•°:")
    print(f"   - æ€»episodes: {NUM_EPISODES:,}")
    print(f"   - Îµå€¼: {EPSILON}")
    print(f"   - éšæœºç§å­: {SEED}")
    print(f"   - æ—¥å¿—é—´éš”: {LOG_EVERY:,}")
    print()

    Q, visits = mc_control_exploring_starts(
        num_episodes=NUM_EPISODES,
        epsilon=EPSILON,
        seed=SEED,
        log_every=LOG_EVERY,
        recent_window=RECENT_WINDOW,
        verbose=True
    )

    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±å­¦ä¹ äº† {len(Q)} ä¸ªçŠ¶æ€çš„ä»·å€¼å‡½æ•°")
    print(f"ğŸ“ˆ çŠ¶æ€-åŠ¨ä½œå¯¹æ€»è®¿é—®æ¬¡æ•°: {sum(visits.values()):,}")
    
    policy = derive_greedy_policy(Q)
    print_policy(policy)
    
    print("\nğŸ’¡ ç­–ç•¥è§£è¯»:")
    print("- é«˜ç‚¹æ•°(17-21)æ—¶é€šå¸¸åœç‰Œï¼Œé¿å…çˆ†ç‰Œ")
    print("- ä½ç‚¹æ•°(12-16)æ—¶æ ¹æ®åº„å®¶æ˜ç‰Œå†³å®šè¦ç‰Œæˆ–åœç‰Œ")
    print("- æœ‰è½¯Aæ—¶ç­–ç•¥æ›´æ¿€è¿›ï¼Œå› ä¸ºAå¯ä»¥æŒ‰1æˆ–11è®¡ç®—")
